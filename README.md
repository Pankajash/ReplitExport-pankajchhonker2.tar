# ğŸš€ AdaptivePuzzleMind  
### Real-Time Data Engineering Pipeline (Kafka + Spark + Python)

AdaptivePuzzleMind is a real-time data pipeline designed to simulate, ingest, process, and analyze high-volume event streams using **Apache Kafka**, **PySpark**, and **Python**.  
This project demonstrates core **Data Engineering**, **ETL processing**, and **Distributed System** skills using industry-standard tools.

---

## ğŸ§  Project Purpose
This project was created to showcase hands-on experience with real-time data pipelines, message streaming, distributed batch processing, and analytics workflows â€” all essential skills for Data Engineering and Backend roles.

---

## ğŸ”¥ Key Features
- âš¡ **Real-time event generation** using Kafka Producers  
- ğŸ”„ **Stream ingestion** with Kafka Consumers  
- ğŸ—ƒ **Raw data storage** in JSON format  
- ğŸ” **Batch processing & aggregation** using PySpark  
- ğŸ§® Analytics on event volume, clicks, cost  
- ğŸ³ **Docker Compose** setup for Kafka + Zookeeper  
- ğŸ“ Clean ETL folder structure  
- ğŸ§ª Sample dataset included to test Spark without Kafka  

---

## ğŸ— Architecture Overview

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Event Generator â”‚
â”‚ (Kafka Producer) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ JSON Events
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka Topic â”‚
â”‚ "events" â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka Consumer â”‚
â”‚ Writes to /data/ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Raw JSON Files â”‚
â”‚ /data/raw â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PySpark Batch Job â”‚
â”‚ Aggregates + ETL â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Processed Data â”‚
â”‚ /data/processedâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


---

## ğŸ“‚ Folder Structure



AdaptivePuzzleMind/
â”‚
â”œâ”€â”€ producer.py
â”œâ”€â”€ consumer.py
â”œâ”€â”€ spark_job.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ sample_data/
â”‚ â””â”€â”€ events.json
â”‚
â””â”€â”€ data/
â”œâ”€â”€ raw/
â””â”€â”€ processed/


---

## ğŸ›  Tech Stack
- **Python 3**
- **Apache Kafka**
- **Kafka Python Client**
- **Apache Spark (PySpark)**
- **Docker & Docker Compose**
- **JSON Serialization**
- **Distributed Systems Concepts**

---

## âš™ï¸ Installation & Setup

### **1ï¸âƒ£ Install Dependencies**
```bash
pip install -r requirements.txt

2ï¸âƒ£ Start Kafka & Zookeeper
docker-compose up -d

3ï¸âƒ£ Start Producer
python3 producer.py --topic events --brokers localhost:9092 --rate 20

4ï¸âƒ£ Start Consumer
python3 consumer.py --topic events --brokers localhost:9092 --out_dir data/raw

5ï¸âƒ£ Run Spark Batch Job
spark-submit spark_job.py data/raw data/processed

ğŸ“Š Output Example

Processed JSON output:

{
  "ad_id": "ad_12",
  "events": 147,
  "clicks": 41,
  "total_cost": 82.37
}

ğŸ§ª Testing Without Kafka

You can test Spark using sample data:

cp sample_data/events.json data/raw/
spark-submit spark_job.py data/raw data/processed

ğŸ§© Skills Demonstrated

Real-time streaming pipeline design

Kafka producers/consumers

PySpark ETL and batch processing

Data transformation & aggregation

Distributed system handling

Serialization, retries, partitions

GitHub version control & project documentation

ğŸ‘¨â€ğŸ’» Author

Pankaj Kumar
Data Engineering & Backend Enthusiast

ğŸ“§ Email: pankajchhonker28@gmail.com

ğŸ”— LinkedIn: https://www.linkedin.com/in/pankaj-kumar-19b240231

â­ If you like this project, please star the repository!

---

